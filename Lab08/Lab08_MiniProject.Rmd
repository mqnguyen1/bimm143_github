---
title: "Lab08_Mini_Assignment"
author: "Matthew"
output: github_document
---


In this class we will explore a complete analysis using the unsupervised learning techniques 
```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```
Remove the diagnosis column and keep it in a separate vector for later
```{r}
diagnosis <- as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

# Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```
There are 569 obervations

# Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```
212 Malignant Diagnosis 

# Q3. How many variables/features in the data are suffixed with _mean?
First find the column names
```{r}
colnames(wisc.data)
```
Next I need to search within the column names for "_mean" pattern. The 'grep()' function might. 
```{r}
grep("_mean",colnames(wisc.data))
```
```{r}
inds <- grep("_mean",colnames(wisc.data))
length(inds)
```
There are 10 variables with suffix _mean

# Q. How many dimesnions are in this dataset?
```{r}
ncol(wisc.data)
```

# Principal Component Analysis 

First do we need to scale the data before PCA or not 

```{r}
round(apply(wisc.data,2,sd),3)
```

Looks like we need to scale. 

```{r}
wisc.pr <- prcomp(wisc.data,scale=TRUE)
summary(wisc.pr)
```
# Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27% 

# Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs capture 72% 

# Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs capture 91%


# PC Plot 

# Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
```{r}
biplot(wisc.pr)
```
It is very hard to understand because there is a lot of data overlapping each other as well as the labels for this graph. This compares the PC1 and PC2.

We need to make our plot of PC1 vs PC2 (aka score plot, PC-plot, etc.)/ The main result of PCA... 
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=diagnosis)
```

```{r}
library(ggplot2)
pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <- diagnosis 

ggplot(pc)+
  aes(PC1,PC2,col=diagnosis)+
  geom_point()
```

# Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3],col=diagnosis)
```
The samples for the benign and malignant seem to be more mixed in with each other. 


# Calculate variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

# Variance explained by each principal component: pve

```{r}
pve <- pr.var / sum(pr.var)
head(pve)
```

# Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


#Examine the PC loadings
How much do the original variables contribute to the new PCs that we have calculated? To get at this data we can look at the '$rotation' componentn of the returned PCA object 

```{r}
head(wisc.pr$rotation[,1:3])
```

Focus in on PC1

```{r}
head(wisc.pr$rotation[,1])
```

#Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation["concave.points_mean",1]
```

This is a complicated mix of variables that go together to make up PC1 - ie there are many of the original variables that together contribute highly to PC1. 

#Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
Atleast 3

```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings)+
  aes(PC1,rownames(loadings))+
  geom_col()
```

# Hierarchical Clustering 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```
First we scale the data, then distance matrix, then hclust
```{r}
wisc.hclust <- hclust(dist(scale(data.scaled)))
```

```{r}
plot(wisc.hclust)
```

Cut this tree to yield our cluster membership vector with 'cutree()' function. 

```{r}
grps <- cutree(wisc.hclust, h=19)
table(grps)
```

```{r}
table(grps,diagnosis)
```
# Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="red", lty=2)
```
At a height of 19

# Combine methods: PCA and HCLUST

My PCA results were interesting as they showed a separation of M and B samples along PC1. 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

# Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
Increasing the height would mean that the cluster vs diagnoses match would become better. 

I want to cluster my PCA results - that is use 'wisc.pr$x' as input to 'hclust()'

# Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Try clustering in 3 PCs, that is PC1, PC2, and PC3 as input 
```{r}
d <- dist(wisc.pr$x[,1:3])

wisc.pr.clust <- hclust(d,method="ward.D2")
```

```{r}
plot(wisc.pr.clust)
```
I like ward.D2 since that data seems more spread out and clear/organized so it is easier to see where the lines run to on the graph. 

# Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
Let's cut this tree into two groups/clusters

```{r}
grps <- cutree(wisc.pr.clust,k=2)
table(grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

How well do the two clusters separate the M and B diagnoses 

```{r}
table(grps,diagnosis)

```


# Q15. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
(179+333)/nrow(wisc.data)
```
Around 90% 
